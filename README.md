# LLM API Server

ä¸€ä¸ªåŸºäº FastAPI çš„å¤§è¯­è¨€æ¨¡å‹ API æœåŠ¡å™¨ï¼Œæä¾›ä¸ OpenAI API å…¼å®¹çš„æ¥å£ï¼Œæ”¯æŒæ ‡å‡† LLM å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLï¼‰ã€‚

## âœ¨ ç‰¹æ€§

- ğŸ”Œ **OpenAI API å…¼å®¹**ï¼šå®Œå…¨å…¼å®¹ OpenAI Chat Completions API è§„èŒƒ
- ğŸ¯ **åŒæ¨¡å‹æ”¯æŒ**ï¼šåŒæ—¶æ”¯æŒçº¯æ–‡æœ¬ LLM å’Œå¤šæ¨¡æ€è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLï¼‰
- ğŸš€ **æµå¼å“åº”**ï¼šæ”¯æŒæµå¼ï¼ˆStreamingï¼‰å’Œéæµå¼å“åº”æ¨¡å¼
- ğŸ”’ **API å¯†é’¥è®¤è¯**ï¼šå†…ç½® Bearer Token è®¤è¯æœºåˆ¶
- ğŸ—ï¸ **æ¨¡å—åŒ–è®¾è®¡**ï¼šæ¸…æ™°çš„ä»£ç æ¶æ„ï¼Œæ˜“äºæ‰©å±•å’Œç»´æŠ¤
- âš¡ **é«˜æ€§èƒ½**ï¼šåŸºäº llama-cpp-python å’Œ FastAPI æ„å»º
- ğŸ¨ **å¤šæ¨¡æ€æ”¯æŒ**ï¼šVL æ¨¡å¼ä¸‹æ”¯æŒå›¾æ–‡æ··åˆè¾“å…¥

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [å®‰è£…](#å®‰è£…)
- [é…ç½®](#é…ç½®)
- [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
- [API æ–‡æ¡£](#api-æ–‡æ¡£)
- [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
- [ç¤ºä¾‹](#ç¤ºä¾‹)

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- CUDAï¼ˆå¯é€‰ï¼Œç”¨äº GPU åŠ é€Ÿï¼‰
- è¶³å¤Ÿçš„å†…å­˜å’Œå­˜å‚¨ç©ºé—´ç”¨äºåŠ è½½æ¨¡å‹

### å®‰è£…

1. å…‹éš†ä»“åº“ï¼š

```bash
git clone https://github.com/MrChenLearnSpace/LLMApiServer.git
cd LLMApiServer
```

2. å®‰è£…ä¾èµ–ï¼š

```bash
# åŸºç¡€ä¾èµ–
pip install "fastapi[all]" uvicorn

# å®‰è£… llama-cpp-pythonï¼ˆCPU ç‰ˆæœ¬ï¼‰
pip install llama-cpp-python

# æˆ–è€…å®‰è£… GPU ç‰ˆæœ¬ï¼ˆæ”¯æŒ CUDAï¼‰
CMAKE_ARGS="-DLLAMA_CUDA=on" FORCE_CMAKE=1 pip install llama-cpp-python
```

3. ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼š

å°†æ‚¨çš„ GGUF æ ¼å¼æ¨¡å‹æ–‡ä»¶æ”¾ç½®åˆ°é¡¹ç›®ç›®å½•ä¸­ï¼Œæˆ–è€…æŒ‡å®šæ¨¡å‹è·¯å¾„ã€‚

### å¯åŠ¨æœåŠ¡

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

## âš™ï¸ é…ç½®

æœåŠ¡å™¨é€šè¿‡ç¯å¢ƒå˜é‡è¿›è¡Œé…ç½®ï¼š

### ç¯å¢ƒå˜é‡

| å˜é‡å | è¯´æ˜ | é»˜è®¤å€¼ | å¿…éœ€ |
|--------|------|--------|------|
| `MODEL_TYPE` | æ¨¡å‹ç±»å‹ï¼š`LLM` æˆ– `VL` | `VL` | å¦ |
| `MODEL_PATH` | GGUF æ¨¡å‹æ–‡ä»¶è·¯å¾„ | `./Qwen3-VL-4B-Instruct-UD-IQ1_M.gguf` | å¦ |
| `MMPROJ_PATH` | VL æ¨¡å‹çš„ MMProj æ–‡ä»¶è·¯å¾„ | `./mmproj-BF16.gguf` | VL æ¨¡å¼æ—¶å¿…éœ€ |
| `MODEL_NAME` | æ¨¡å‹åç§°ï¼ˆAPI å“åº”ä¸­ä½¿ç”¨ï¼‰ | `default-model` | å¦ |

### é…ç½®ç¤ºä¾‹

#### å¯åŠ¨ LLM æ¨¡å¼ï¼š

```bash
export MODEL_TYPE=LLM
export MODEL_PATH=/path/to/your/model.gguf
export MODEL_NAME=my-llm-model
uvicorn main:app --host 0.0.0.0 --port 8000
```

#### å¯åŠ¨ VL æ¨¡å¼ï¼ˆå¤šæ¨¡æ€ï¼‰ï¼š

```bash
export MODEL_TYPE=VL
export MODEL_PATH=/path/to/your/vl-model.gguf
export MMPROJ_PATH=/path/to/your/mmproj.gguf
export MODEL_NAME=my-vl-model
uvicorn main:app --host 0.0.0.0 --port 8000
```

### API å¯†é’¥é…ç½®

é»˜è®¤çš„ API å¯†é’¥å®šä¹‰åœ¨ `main.py` ä¸­çš„ `VALID_API_KEYS` é›†åˆä¸­ï¼š

```python
VALID_API_KEYS = {"aa1234567", "another-valid-key-for-testing"}
```

**âš ï¸ ç”Ÿäº§ç¯å¢ƒå»ºè®®**ï¼š
- ä¿®æ”¹é»˜è®¤å¯†é’¥
- ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†å¯†é’¥
- å®æ–½æ›´ä¸¥æ ¼çš„è®¤è¯æœºåˆ¶

## ğŸ“– ä½¿ç”¨æ–¹æ³•

### Python ç¤ºä¾‹

#### åŸºç¡€å¯¹è¯ï¼ˆéæµå¼ï¼‰

```python
import requests

url = "http://localhost:8000/v1/chat/completions"
headers = {
    "Authorization": "Bearer aa1234567",
    "Content-Type": "application/json"
}

data = {
    "model": "default-model",
    "messages": [
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"}
    ],
    "temperature": 0.7,
    "max_tokens": 2048,
    "stream": False
}

response = requests.post(url, headers=headers, json=data)
print(response.json()["choices"][0]["message"]["content"])
```

#### æµå¼å“åº”

```python
import requests

url = "http://localhost:8000/v1/chat/completions"
headers = {
    "Authorization": "Bearer aa1234567",
    "Content-Type": "application/json"
}

data = {
    "model": "default-model",
    "messages": [
        {"role": "user", "content": "è®²ä¸ªæ•…äº‹"}
    ],
    "stream": True
}

response = requests.post(url, headers=headers, json=data, stream=True)

for line in response.iter_lines():
    if line:
        line = line.decode('utf-8')
        if line.startswith('data: '):
            chunk = line[6:]
            if chunk != '[DONE]':
                import json
                data = json.loads(chunk)
                content = data["choices"][0]["delta"].get("content", "")
                print(content, end="", flush=True)
```

#### å¤šæ¨¡æ€è¾“å…¥ï¼ˆVL æ¨¡å¼ï¼‰

```python
import base64
import requests

# è¯»å–å¹¶ç¼–ç å›¾ç‰‡
with open("image.jpg", "rb") as f:
    image_base64 = base64.b64encode(f.read()).decode('utf-8')

url = "http://localhost:8000/v1/chat/completions"
headers = {
    "Authorization": "Bearer aa1234567",
    "Content-Type": "application/json"
}

data = {
    "model": "default-model",
    "messages": [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "è¿™å¼ å›¾ç‰‡é‡Œæœ‰ä»€ä¹ˆï¼Ÿ"},
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/jpeg;base64,{image_base64}"
                    }
                }
            ]
        }
    ],
    "temperature": 0.7,
    "max_tokens": 2048
}

response = requests.post(url, headers=headers, json=data)
print(response.json()["choices"][0]["message"]["content"])
```

### cURL ç¤ºä¾‹

```bash
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Authorization: Bearer aa1234567" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "default-model",
    "messages": [
      {"role": "user", "content": "ä½ å¥½"}
    ],
    "temperature": 0.7,
    "max_tokens": 2048,
    "stream": false
  }'
```

## ğŸ“š API æ–‡æ¡£

### ç«¯ç‚¹

#### POST `/v1/chat/completions`

åˆ›å»ºèŠå¤©è¡¥å…¨ã€‚

**è¯·æ±‚å¤´ï¼š**
- `Authorization: Bearer <api_key>` - å¿…éœ€

**è¯·æ±‚ä½“å‚æ•°ï¼š**

| å‚æ•° | ç±»å‹ | å¿…éœ€ | è¯´æ˜ |
|------|------|------|------|
| `model` | string | æ˜¯ | æ¨¡å‹åç§° |
| `messages` | array | æ˜¯ | æ¶ˆæ¯æ•°ç»„ |
| `temperature` | float | å¦ | æ¸©åº¦å‚æ•°ï¼ˆ0.0-2.0ï¼‰ï¼Œé»˜è®¤ 0.7 |
| `max_tokens` | integer | å¦ | æœ€å¤§ç”Ÿæˆ token æ•°ï¼Œé»˜è®¤ 2048 |
| `stream` | boolean | å¦ | æ˜¯å¦å¯ç”¨æµå¼å“åº”ï¼Œé»˜è®¤ false |

**æ¶ˆæ¯æ ¼å¼ï¼š**

çº¯æ–‡æœ¬æ¶ˆæ¯ï¼š
```json
{
  "role": "user",
  "content": "æ¶ˆæ¯å†…å®¹"
}
```

å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆVL æ¨¡å¼ï¼‰ï¼š
```json
{
  "role": "user",
  "content": [
    {"type": "text", "text": "æ–‡æœ¬å†…å®¹"},
    {
      "type": "image_url",
      "image_url": {
        "url": "data:image/jpeg;base64,<base64_string>"
      }
    }
  ]
}
```

**å“åº”æ ¼å¼ï¼š**

éæµå¼å“åº”ï¼š
```json
{
  "id": "chatcmpl-xxxxx",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "default-model",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "å›å¤å†…å®¹"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20,
    "total_tokens": 30
  }
}
```

æµå¼å“åº”ï¼ˆSSE æ ¼å¼ï¼‰ï¼š
```
data: {"id":"chatcmpl-xxxxx","object":"chat.completion.chunk","created":1234567890,"model":"default-model","choices":[{"index":0,"delta":{"content":"å†…å®¹"},"finish_reason":null}]}

data: [DONE]
```

## ğŸ—‚ï¸ é¡¹ç›®ç»“æ„

```
LLMApiServer/
â”œâ”€â”€ main.py              # ä¸»åº”ç”¨å…¥å£ï¼ŒåŒ…å« FastAPI åº”ç”¨å’Œè·¯ç”±
â”œâ”€â”€ api_models.py        # Pydantic æ•°æ®æ¨¡å‹å®šä¹‰
â”œâ”€â”€ model_wrapper.py     # æ¨¡å‹å°è£…ç±»ï¼ˆLLMWrapper å’Œ VLWrapperï¼‰
â”œâ”€â”€ mainv1.0.0.py       # v1.0.0 ç‰ˆæœ¬çš„ä¸»æ–‡ä»¶ï¼ˆå†å²ç‰ˆæœ¬ï¼‰
â”œâ”€â”€ deploy.ipynb        # éƒ¨ç½²ç¬”è®°æœ¬
â”œâ”€â”€ code-server.sh      # Code Server å®‰è£…è„šæœ¬
â”œâ”€â”€ .gitignore          # Git å¿½ç•¥æ–‡ä»¶é…ç½®
â””â”€â”€ README.md           # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

### æ ¸å¿ƒæ¨¡å—è¯´æ˜

#### `main.py`
- FastAPI åº”ç”¨åˆå§‹åŒ–
- ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆæ¨¡å‹åŠ è½½/å¸è½½ï¼‰
- API è·¯ç”±å®šä¹‰
- è®¤è¯ä¸­é—´ä»¶

#### `api_models.py`
- è¯·æ±‚/å“åº”æ•°æ®æ¨¡å‹
- ç¬¦åˆ OpenAI API è§„èŒƒçš„ Pydantic æ¨¡å‹
- æ”¯æŒæµå¼å’Œéæµå¼å“åº”æ ¼å¼

#### `model_wrapper.py`
- `ModelWrapper`: æŠ½è±¡åŸºç±»
- `LLMWrapper`: æ ‡å‡† LLM å°è£…
- `VLWrapper`: è§†è§‰è¯­è¨€æ¨¡å‹å°è£…
- æµå¼ç”Ÿæˆå™¨å®ç°

## ğŸ”§ é«˜çº§é…ç½®

### æ¨¡å‹å‚æ•°è°ƒæ•´

åœ¨ `main.py` ä¸­å¯ä»¥è°ƒæ•´æ¨¡å‹åŠ è½½å‚æ•°ï¼š

```python
model_kwargs = {
    "n_ctx": 8192,        # ä¸Šä¸‹æ–‡çª—å£å¤§å°
    "n_gpu_layers": -1,   # GPU å±‚æ•°ï¼ˆ-1 è¡¨ç¤ºå…¨éƒ¨ä½¿ç”¨ GPUï¼‰
    "verbose": False      # æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
}
```

### è‡ªå®šä¹‰æ¨¡å‹å°è£…

å¦‚éœ€æ”¯æŒå…¶ä»–æ¨¡å‹ç±»å‹ï¼Œå¯ä»¥ç»§æ‰¿ `ModelWrapper` ç±»ï¼š

```python
class CustomModelWrapper(ModelWrapper):
    def _load_model(self, **kwargs) -> Llama:
        # è‡ªå®šä¹‰æ¨¡å‹åŠ è½½é€»è¾‘
        pass
    
    def _prepare_messages(self, messages: List[ChatMessage]) -> List[dict]:
        # è‡ªå®šä¹‰æ¶ˆæ¯é¢„å¤„ç†é€»è¾‘
        pass
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹åŠ è½½å¤±è´¥**
   - æ£€æŸ¥æ¨¡å‹æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®
   - ç¡®è®¤æœ‰è¶³å¤Ÿçš„å†…å­˜
   - æŸ¥çœ‹æœåŠ¡å™¨æ—¥å¿—è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯

2. **CUDA ç›¸å…³é”™è¯¯**
   - ç¡®è®¤ CUDA ç‰ˆæœ¬å…¼å®¹
   - é‡æ–°å®‰è£…æ”¯æŒ CUDA çš„ llama-cpp-python

3. **è®¤è¯å¤±è´¥**
   - ç¡®è®¤ä½¿ç”¨æ­£ç¡®çš„ API å¯†é’¥
   - æ£€æŸ¥ Authorization å¤´æ ¼å¼ï¼š`Bearer <api_key>`

4. **VL æ¨¡å¼ä¸‹å›¾ç‰‡æ— æ³•è¯†åˆ«**
   - ç¡®ä¿æä¾›äº† MMPROJ_PATH
   - æ£€æŸ¥å›¾ç‰‡ base64 ç¼–ç æ ¼å¼æ­£ç¡®
   - ç¡®è®¤æ¨¡å‹æ”¯æŒè§†è§‰è¾“å…¥

## ğŸ“ ç‰ˆæœ¬å†å²

- **v1.0.0**: åŸºç¡€ç‰ˆæœ¬ï¼Œæ”¯æŒæ ‡å‡† LLM
- **å½“å‰ç‰ˆæœ¬**: å¢åŠ å¤šæ¨¡æ€æ”¯æŒï¼Œæ¨¡å—åŒ–é‡æ„ï¼Œå®Œå–„ API å…¼å®¹æ€§

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨å¼€æºè®¸å¯è¯ï¼Œå…·ä½“è¯·æŸ¥çœ‹ LICENSE æ–‡ä»¶ã€‚

## ğŸ™ è‡´è°¢

- [FastAPI](https://fastapi.tiangolo.com/) - ç°ä»£åŒ–çš„ Python Web æ¡†æ¶
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) - llama.cpp çš„ Python ç»‘å®š
- OpenAI - API è§„èŒƒå‚è€ƒ

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ GitHub Issues è”ç³»ã€‚

---

**æ³¨æ„**ï¼šæœ¬é¡¹ç›®ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ï¼Œè¯·éµå®ˆç›¸å…³æ¨¡å‹çš„ä½¿ç”¨åè®®å’Œé™åˆ¶ã€‚
