{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e80744-e710-4c6b-9688-589a1ff05931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pip 25.2 from /root/miniconda3/envs/llama/lib/python3.10/site-packages/pip (python 3.10)\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting llama-cpp-python\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/e4/b4/c8cd17629ced0b9644a71d399a91145aedef109c0333443bef015e45b704/llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0ma \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25h  Running command pip subprocess to install build dependencies\n",
      "  Using pip 25.2 from /root/miniconda3/envs/llama/lib/python3.10/site-packages/pip (python 3.10)\n",
      "  Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "  Collecting scikit-build-core>=0.9.2 (from scikit-build-core[pyproject]>=0.9.2)\n",
      "    Using cached https://mirrors.aliyun.com/pypi/packages/43/49/ec16b3db6893db788ae35f98506ff5a9c25dca7eb18cc38ada8a4c1dc944/scikit_build_core-0.11.6-py3-none-any.whl (185 kB)\n",
      "  Collecting exceptiongroup>=1.0 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)\n",
      "    Using cached https://mirrors.aliyun.com/pypi/packages/36/f4/c6e662dade71f56cd2f3735141b265c3c79293c109549c1e6933b0651ffc/exceptiongroup-1.3.0-py3-none-any.whl (16 kB)\n",
      "  Collecting packaging>=23.2 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)\n",
      "    Downloading https://mirrors.aliyun.com/pypi/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl (66 kB)\n",
      "  Collecting pathspec>=0.10.1 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)\n",
      "    Downloading https://mirrors.aliyun.com/pypi/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "  Collecting tomli>=1.2.2 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)\n",
      "    Downloading https://mirrors.aliyun.com/pypi/packages/77/b8/0135fadc89e73be292b473cb820b4f5a08197779206b33191e801feeae40/tomli-2.3.0-py3-none-any.whl (14 kB)\n",
      "  Collecting typing-extensions>=4.6.0 (from exceptiongroup>=1.0->scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)\n",
      "    Using cached https://mirrors.aliyun.com/pypi/packages/18/67/36e9267722cc04a6b9f15c7f3441c2363321a3ea07da7ae0c0707beb2a9c/typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "  Installing collected packages: typing-extensions, tomli, pathspec, packaging, exceptiongroup, scikit-build-core\n",
      "\n",
      "  Successfully installed exceptiongroup-1.3.0 packaging-25.0 pathspec-0.12.1 scikit-build-core-0.11.6 tomli-2.3.0 typing-extensions-4.15.0\n",
      "  WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command Getting requirements to build wheel\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command pip subprocess to install backend dependencies\n",
      "  Using pip 25.2 from /root/miniconda3/envs/llama/lib/python3.10/site-packages/pip (python 3.10)\n",
      "  Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "  Collecting ninja>=1.5\n",
      "    Using cached https://mirrors.aliyun.com/pypi/packages/ed/de/0e6edf44d6a04dabd0318a519125ed0415ce437ad5a1ec9b9be03d9048cf/ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "  Collecting cmake>=3.21\n",
      "    Using cached https://mirrors.aliyun.com/pypi/packages/f3/56/0fc4d83f212cef10b7bbf6c5043e4582af80ad2aef6905e0dc33fbf68b11/cmake-4.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (29.7 MB)\n",
      "  Installing collected packages: ninja, cmake\n",
      "    changing mode of /tmp/pip-build-env-0o_96hoh/normal/bin/ccmake to 755\n",
      "    changing mode of /tmp/pip-build-env-0o_96hoh/normal/bin/cmake to 755\n",
      "    changing mode of /tmp/pip-build-env-0o_96hoh/normal/bin/cpack to 755\n",
      "    changing mode of /tmp/pip-build-env-0o_96hoh/normal/bin/ctest to 755\n",
      "\n",
      "  Successfully installed cmake-4.1.2 ninja-1.13.0\n",
      "  WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Running command Preparing metadata (pyproject.toml)\n",
      "  *** scikit-build-core 0.11.6 using CMake 4.1.2 (metadata_wheel)\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from llama-cpp-python) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from llama-cpp-python) (2.2.6)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Running command Building wheel for llama-cpp-python (pyproject.toml)\n",
      "  *** scikit-build-core 0.11.6 using CMake 4.1.2 (wheel)\n",
      "  *** Configuring CMake...\n",
      "  loading initial cache file /tmp/tmpg0c2j_wj/build/CMakeInit.txt\n",
      "  -- The C compiler identification is GNU 9.4.0\n",
      "  -- The CXX compiler identification is GNU 9.4.0\n",
      "  -- Detecting C compiler ABI info\n",
      "  -- Detecting C compiler ABI info - done\n",
      "  -- Check for working C compiler: /usr/bin/gcc - skipped\n",
      "  -- Detecting C compile features\n",
      "  -- Detecting C compile features - done\n",
      "  -- Detecting CXX compiler ABI info\n",
      "  -- Detecting CXX compiler ABI info - done\n",
      "  -- Check for working CXX compiler: /usr/bin/g++ - skipped\n",
      "  -- Detecting CXX compile features\n",
      "  -- Detecting CXX compile features - done\n",
      "  CMAKE_BUILD_TYPE=Release\n",
      "  -- Found Git: /usr/bin/git (found version \"2.25.1\")\n",
      "  CMake Warning at vendor/llama.cpp/CMakeLists.txt:118 (message):\n",
      "    LLAMA_CUDA is deprecated and will be removed in the future.\n",
      "\n",
      "    Use GGML_CUDA instead\n",
      "\n",
      "  Call Stack (most recent call first):\n",
      "    vendor/llama.cpp/CMakeLists.txt:124 (llama_option_depr)\n",
      "\n",
      "\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
      "  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
      "  -- Found Threads: TRUE\n",
      "  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "  -- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "  -- GGML_SYSTEM_ARCH: x86\n",
      "  -- Including CPU backend\n",
      "  -- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
      "  -- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
      "  -- Found OpenMP: TRUE (found version \"4.5\")\n",
      "  -- x86 detected\n",
      "  -- Adding CPU backend variant ggml-cpu: -march=native\n",
      "  -- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"11.4.100\")\n",
      "  -- CUDA Toolkit found\n",
      "  -- Using CUDA architectures: 50-virtual;61-virtual;70-virtual;75-virtual;80-virtual;86-real\n",
      "  -- The CUDA compiler identification is NVIDIA 11.4.100 with host compiler GNU 9.4.0\n",
      "  -- Detecting CUDA compiler ABI info\n",
      "  -- Detecting CUDA compiler ABI info - done\n",
      "  -- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
      "  -- Detecting CUDA compile features\n",
      "  -- Detecting CUDA compile features - done\n",
      "  -- CUDA host compiler is GNU 9.4.0\n",
      "  -- Including CUDA backend\n",
      "  -- ggml version: 0.0.1\n",
      "  -- ggml commit:  4227c9b\n",
      "  CMake Warning (dev) at CMakeLists.txt:13 (install):\n",
      "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:108 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\n",
      "  CMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:108 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\n",
      "  CMake Warning (dev) at CMakeLists.txt:13 (install):\n",
      "    Target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:109 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\n",
      "  CMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "    Target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:109 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\n",
      "  CMake Warning (dev) at CMakeLists.txt:13 (install):\n",
      "    Target mtmd has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:162 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\n",
      "  CMake Warning (dev) at CMakeLists.txt:21 (install):\n",
      "    Target mtmd has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.\n",
      "  Call Stack (most recent call first):\n",
      "    CMakeLists.txt:162 (llama_cpp_python_install_target)\n",
      "  This warning is for project developers.  Use -Wno-dev to suppress it.\n",
      "\n",
      "  -- Configuring done (6.2s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -- Generating done (0.1s)\n",
      "  -- Build files have been written to: /tmp/tmpg0c2j_wj/build\n",
      "  *** Building project with Ninja...\n",
      "  Change Dir: '/tmp/tmpg0c2j_wj/build'\n",
      "\n",
      "  Run Build Command(s): ninja -v\n",
      "  [1/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/hbm.cpp\n",
      "  [2/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml.cpp\n",
      "  [3/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-threading.cpp\n",
      "  [4/183] /usr/bin/gcc  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-alloc.c\n",
      "  [5/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/traits.cpp\n",
      "  [6/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/vec.cpp\n",
      "  [7/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp\n",
      "  [8/183] /usr/bin/gcc  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [9/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp\n",
      "  [10/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp\n",
      "  [11/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-backend.cpp\n",
      "  [12/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-opt.cpp\n",
      "  [13/183] /usr/bin/gcc  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/quants.c\n",
      "  [14/183] /usr/bin/gcc  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml.c\n",
      "  [15/183] /usr/bin/gcc  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/arch/x86/quants.c\n",
      "  [16/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/binary-ops.cpp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [17/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/unary-ops.cpp\n",
      "  [18/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp\n",
      "  [19/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/gguf.cpp\n",
      "  [20/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/arch/x86/repack.cpp\n",
      "  [21/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp\n",
      "  [22/183] /usr/bin/gcc  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BUILD -DGGML_COMMIT=\\\"4227c9b\\\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\\\"0.0.1\\\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-quants.c\n",
      "  [23/183] /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cpu/ops.cpp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [24/183] : && /usr/bin/g++  -pthread -B /root/miniconda3/envs/llama/compiler_compat -fPIC -O3 -DNDEBUG  -shared -Wl,--dependency-file=vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/link.d -Wl,-soname,libggml-base.so -o bin/libggml-base.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o  -Wl,-rpath,\"\\$ORIGIN\"  -lm && :\n",
      "  [25/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/arange.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [26/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/argmax.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [27/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/clamp.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [28/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/add-id.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [29/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/acc.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [30/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/argsort.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [31/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/conv2d-dw.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [32/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [33/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/concat.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [34/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/conv2d-transpose.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [35/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/count-equal.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [36/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [37/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/binbcast.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [38/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/diagmask.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [39/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=half]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=nv_bfloat16]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=float]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=half]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=float]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=half]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=nv_bfloat16]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=float]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=half]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=float]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=half]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=nv_bfloat16]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=float]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=half]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=float]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=half]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=nv_bfloat16]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=float]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=half]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=float]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=half]\"\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=float]\"\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=float, dst_t=half]\"\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=half, dst_t=float]\"\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/cpy-utils.cuh(216): here\r\n",
      "              instantiation of \"void cpy_1_flt<src_t,dst_t>(const char *, char *) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "\r\n",
      "  [40/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\r\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(769): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(781): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(790): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(802): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(811): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(823): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(769): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(781): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(790): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(802): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(811): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(823): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(769): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(781): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(790): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(802): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(811): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(823): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(769): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(781): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(790): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(802): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(811): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(823): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(769): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(781): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(790): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(802): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(811): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(823): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\r\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\r\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(769): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(781): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(790): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(802): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=half, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=half, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(811): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(633): here\n",
      "              instantiation of \"void convert_unary<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=nv_bfloat16, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(642): here\n",
      "              instantiation of \"void convert_unary_cuda<src_t,dst_t>(const void *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, cudaStream_t) [with src_t=nv_bfloat16, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu(823): here\n",
      "\n",
      "  [41/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/fattn.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [42/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/mean.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [43/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/im2col.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [44/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/gla.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [45/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/mmq.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [46/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\n",
      "            detected during:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=float, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(215): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=int32_t, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(219): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=nv_bfloat16, dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(178): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(223): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=half]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=half, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(166): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\r\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=float, dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(170): here\r\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=int32_t]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(66): here\r\n",
      "              instantiation of \"void k_get_rows_float(const src0_t *, const int32_t *, dst_t *, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(153): here\n",
      "              instantiation of \"void get_rows_cuda_float(const src0_t *, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src0_t=int32_t, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(174): here\n",
      "              instantiation of \"void ggml_cuda_get_rows_switch_src0_type(const void *, ggml_type, const int32_t *, dst_t *, int64_t, size_t, size_t, size_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu(227): here\n",
      "\n",
      "  [47/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [48/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [49/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [50/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/opt-step-sgd.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [51/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/norm.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [52/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/out-prod.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [53/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/pad.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [54/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/pool2d.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [55/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/roll.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [56/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/quantize.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [57/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [58/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/scale.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [59/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/softcap.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [60/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/rope.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [61/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/sum.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [62/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(186): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(196): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(186): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(196): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(186): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(196): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(186): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(196): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(186): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(196): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=half, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=half]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(186): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cuh(44): warning: missing return statement at end of non-void function \"ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"dst_t ggml_cuda_cast<dst_t,src_t>(src_t) [with dst_t=nv_bfloat16, src_t=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(115): here\n",
      "              instantiation of \"void k_set_rows(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(154): here\n",
      "              instantiation of \"void set_rows_cuda(const src_t *, const int64_t *, dst_t *, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, int64_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, size_t, cudaStream_t) [with src_t=float, dst_t=nv_bfloat16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu(196): here\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [63/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/ssm-conv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [64/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/sumrows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [65/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/tsembd.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [66/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/softmax.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [67/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/upscale.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [68/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/unary.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [69/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/ssm-scan.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [70/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/wkv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [71/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  [72/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\n",
      "            detected during:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=4, ncols2=16, nwarps=8, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=4, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu(5): here\n",
      "\n",
      "  [73/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\n",
      "            detected during:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=2, ncols2=16, nwarps=4, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=2, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu(5): here\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [74/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o\r\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=576, DV=512, ncols1=1, ncols2=16, nwarps=2, ntiles=2, use_logit_softcap=false, mla=true]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=576, DV=512, ncols1=1, ncols2=16]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu(5): here\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [75/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\n",
      "            detected during:\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  (932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  (1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  (1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\n",
      "            detected during:\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\n",
      "  (938): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  (1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  (1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\n",
      "            detected during:\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\n",
      "            detected during:\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\n",
      "  (932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\n",
      "  (1354): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  (1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\n",
      "            detected during:\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\n",
      "  (938): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\n",
      "  (1354): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  (1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\n",
      "            detected during:\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\n",
      "  (932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\n",
      "  (1396): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  (1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\n",
      "            detected during:\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\n",
      "  (938): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\n",
      "  (1396): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\n",
      "  (1458): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\n",
      "            detected during:\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\n",
      "  (932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  (1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\n",
      "  (1469): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\n",
      "            detected during:\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\n",
      "  (938): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\n",
      "  (1349): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\n",
      "  (1469): here\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\n",
      "\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\n",
      "            detected during:\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\n",
      "  (932): here\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\n",
      "  (1354): here\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\n",
      "  (1469): here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=1, ncols2=8, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=1, ncols2=8, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=1, ncols2=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu(10): here\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [76/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\r\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=2, ncols2=4, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=2, ncols2=4, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=2, ncols2=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu(10): here\r\n",
      "\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [77/183] /usr/local/cuda/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 \"--generate-code=arch=compute_50,code=[compute_50]\" \"--generate-code=arch=compute_61,code=[compute_61]\" \"--generate-code=arch=compute_70,code=[compute_70]\" \"--generate-code=arch=compute_75,code=[compute_75]\" \"--generate-code=arch=compute_80,code=[compute_80]\" \"--generate-code=arch=compute_86,code=[sm_86]\" -Xcompiler=-fPIC -use_fast_math -extended-lambda -Xcompiler \"-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic\" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\r\n",
      "  nvcc warning : The 'compute_35', 'compute_37', 'compute_50', 'sm_35', 'sm_37' and 'sm_50' architectures are deprecated, and may be removed in a future release (Use -Wno-deprecated-gpu-targets to suppress warning).\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(550): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=8, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=8, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(551): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(134): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_j [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_j(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(598): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(120): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, T>::get_i [with I_=16, J_=16, T=float]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, T>::get_i(int) [with I_=16, J_=16, T=float]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(599): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=4]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=4]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(155): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_i [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_i(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(969): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../mma.cuh(167): warning: missing return statement at end of non-void function \"ggml_cuda_mma::tile<I_, J_, half2>::get_j [with I_=16, J_=8]\"\r\n",
      "            detected during:\r\n",
      "              instantiation of \"int ggml_cuda_mma::tile<I_, J_, half2>::get_j(int) [with I_=16, J_=8]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1163): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=64, DV=64, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=64, DV=64, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(5): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=80, DV=80, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=80, DV=80, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(6): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=96, DV=96, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=96, DV=96, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(7): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=112, DV=112, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=112, DV=112, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(8): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=128, DV=128, ncols1=4, ncols2=2, nwarps=4, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=128, DV=128, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(9): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=false, mla=false]\"\r\n",
      "  (1458): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=false]\"\r\n",
      "  (1349): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=true, is_fixup=false]\"\r\n",
      "  (1354): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=false]\"\r\n",
      "  (932): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/../fattn-mma-f16.cuh(437): warning: variable \"stride_tile_Q\" was declared but never referenced\r\n",
      "            detected during:\r\n",
      "              instantiation of \"void flash_attn_ext_f16_iter<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup,last_iter>(const float2 *, const half2 *, const half2 *, const half2 *, float2 *, float2 *, float, float, float, int, int, int, int, int, half2 *, half2 *, half2 *, half2 *, const tile_B *, tile_C_VKQ *, float *, float *, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true, last_iter=true]\"\r\n",
      "  (938): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16_process_tile<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla,needs_fixup,is_fixup>(const float2 *, const half2 *, const half2 *, const half2 *, const float *, float2 *, float2 *, float, float, float, int, int, int, int, int, int, int, int, int, int) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false, needs_fixup=false, is_fixup=true]\"\r\n",
      "  (1396): here\r\n",
      "              instantiation of \"void flash_attn_ext_f16<DKQ,DV,ncols1,ncols2,nwarps,ntiles,use_logit_softcap,mla>(const char *, const char *, const char *, const char *, const char *, const int *, float *, float2 *, float, float, float, float, uint32_t, float, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t, int32_t, int32_t, int64_t, int32_t, int32_t, int32_t, int32_t, int32_t, int64_t) [with DKQ=256, DV=256, ncols1=4, ncols2=2, nwarps=2, ntiles=1, use_logit_softcap=true, mla=false]\"\r\n",
      "  (1469): here\r\n",
      "              instantiation of \"void ggml_cuda_flash_attn_ext_mma_f16_case<DKQ,DV,ncols1,ncols2>(ggml_backend_cuda_context &, ggml_tensor *) [with DKQ=256, DV=256, ncols1=4, ncols2=2]\"\r\n",
      "  /tmp/pip-install-xd6wma3r/llama-cpp-python_838c8878ec5a4b0f9fd4d70c4277f189/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu(10): here\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# 设置环境变量，确保编译时链接到 CUDA\n",
    "import os\n",
    "os.environ[\"CMAKE_ARGS\"] = \"-DLLAMA_CUDA=on\"\n",
    "os.environ[\"FORCE_CMAKE\"] = \"1\"\n",
    "\n",
    "# 安装 llama-cpp-python\n",
    "# --no-binary :llama-cpp-python: 告诉 pip 不要使用预编译的二进制包，强制从源码编译\n",
    "# --verbose 打印详细日志\n",
    "!pip install llama-cpp-python --no-binary :llama-cpp-python: --no-cache-dir --verbose -i https://mirrors.aliyun.com/pypi/simple/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9878a0e3-7726-45fe-8291-f581939a8663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcc (Ubuntu 9.4.0-1ubuntu1~20.04.3) 9.4.0\n",
      "Copyright (C) 2019 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "export CUDACXX=/usr/local/cuda/bin/nvcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcdf6a8e-3b74-4577-90ab-cb7e265ce89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /root/miniconda3/envs/llama1\n",
      "\n",
      "  added / updated specs:\n",
      "    - gxx_linux-64=10.4.0\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    binutils_impl_linux-64-2.39|       he00db2b_1         4.7 MB  conda-forge\n",
      "    binutils_linux-64-2.39     |      h5fc0e48_13          27 KB  conda-forge\n",
      "    c-ares-1.34.5              |       hb9d3cd8_0         202 KB  conda-forge\n",
      "    curl-8.17.0                |       h4e3cde8_0         182 KB  conda-forge\n",
      "    gcc_impl_linux-64-10.4.0   |      h5231bdf_19        46.7 MB  conda-forge\n",
      "    gcc_linux-64-10.4.0        |      h9215b83_13          29 KB  conda-forge\n",
      "    graalpy-23.0.0             | 0_graalvm_native       326.8 MB  conda-forge\n",
      "    gxx_impl_linux-64-10.4.0   |      h5231bdf_19        11.5 MB  conda-forge\n",
      "    gxx_linux-64-10.4.0        |      h6e491c6_13          27 KB  conda-forge\n",
      "    kernel-headers_linux-64-4.18.0|       he073ed8_8         1.2 MB  conda-forge\n",
      "    keyutils-1.6.3             |       hb9d3cd8_0         131 KB  conda-forge\n",
      "    krb5-1.21.3                |       h659f571_0         1.3 MB  conda-forge\n",
      "    ld_impl_linux-64-2.39      |       hcc3a1bd_1         675 KB  conda-forge\n",
      "    libcurl-8.17.0             |       h4e3cde8_0         450 KB  conda-forge\n",
      "    libedit-3.1.20250104       | pl5321h7949ede_0         132 KB  conda-forge\n",
      "    libev-4.33                 |       hd590300_2         110 KB  conda-forge\n",
      "    libgcc-15.2.0              |       h767d61c_7         803 KB  conda-forge\n",
      "    libgcc-devel_linux-64-10.4.0|      hd38fd1e_19         3.3 MB  conda-forge\n",
      "    libgcc-ng-15.2.0           |       h69a702a_7          29 KB  conda-forge\n",
      "    libgomp-15.2.0             |       h767d61c_7         437 KB  conda-forge\n",
      "    libnghttp2-1.67.0          |       had1ee68_0         651 KB  conda-forge\n",
      "    libsanitizer-10.4.0        |      h5246dfb_19         6.0 MB  conda-forge\n",
      "    libssh2-1.11.1             |       hcf80075_0         298 KB  conda-forge\n",
      "    libstdcxx-15.2.0           |       h8f9b012_7         3.7 MB  conda-forge\n",
      "    libstdcxx-devel_linux-64-10.4.0|      hd38fd1e_19         9.6 MB  conda-forge\n",
      "    libstdcxx-ng-15.2.0        |       h4852527_7          29 KB  conda-forge\n",
      "    openssl-3.5.4              |       h26f9b46_0         3.0 MB  conda-forge\n",
      "    patch-2.8                  |    hb03c661_1002         114 KB  conda-forge\n",
      "    python-3.10.8              |0_native230_graalpy          88 KB  conda-forge\n",
      "    python_abi-3.10            |8_graalpy230_310_native           7 KB  conda-forge\n",
      "    sysroot_linux-64-2.28      |       h4ee821c_8        23.1 MB  conda-forge\n",
      "    zstd-1.5.7                 |       hb8e6e7a_2         554 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       445.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  binutils_impl_lin~ conda-forge/linux-64::binutils_impl_linux-64-2.39-he00db2b_1 \n",
      "  binutils_linux-64  conda-forge/linux-64::binutils_linux-64-2.39-h5fc0e48_13 \n",
      "  c-ares             conda-forge/linux-64::c-ares-1.34.5-hb9d3cd8_0 \n",
      "  curl               conda-forge/linux-64::curl-8.17.0-h4e3cde8_0 \n",
      "  gcc_impl_linux-64  conda-forge/linux-64::gcc_impl_linux-64-10.4.0-h5231bdf_19 \n",
      "  gcc_linux-64       conda-forge/linux-64::gcc_linux-64-10.4.0-h9215b83_13 \n",
      "  graalpy            conda-forge/linux-64::graalpy-23.0.0-0_graalvm_native \n",
      "  gxx_impl_linux-64  conda-forge/linux-64::gxx_impl_linux-64-10.4.0-h5231bdf_19 \n",
      "  gxx_linux-64       conda-forge/linux-64::gxx_linux-64-10.4.0-h6e491c6_13 \n",
      "  kernel-headers_li~ conda-forge/noarch::kernel-headers_linux-64-4.18.0-he073ed8_8 \n",
      "  keyutils           conda-forge/linux-64::keyutils-1.6.3-hb9d3cd8_0 \n",
      "  krb5               conda-forge/linux-64::krb5-1.21.3-h659f571_0 \n",
      "  libcurl            conda-forge/linux-64::libcurl-8.17.0-h4e3cde8_0 \n",
      "  libedit            conda-forge/linux-64::libedit-3.1.20250104-pl5321h7949ede_0 \n",
      "  libev              conda-forge/linux-64::libev-4.33-hd590300_2 \n",
      "  libgcc             conda-forge/linux-64::libgcc-15.2.0-h767d61c_7 \n",
      "  libgcc-devel_linu~ conda-forge/linux-64::libgcc-devel_linux-64-10.4.0-hd38fd1e_19 \n",
      "  libnghttp2         conda-forge/linux-64::libnghttp2-1.67.0-had1ee68_0 \n",
      "  libsanitizer       conda-forge/linux-64::libsanitizer-10.4.0-h5246dfb_19 \n",
      "  libssh2            conda-forge/linux-64::libssh2-1.11.1-hcf80075_0 \n",
      "  libstdcxx          conda-forge/linux-64::libstdcxx-15.2.0-h8f9b012_7 \n",
      "  libstdcxx-devel_l~ conda-forge/linux-64::libstdcxx-devel_linux-64-10.4.0-hd38fd1e_19 \n",
      "  patch              conda-forge/linux-64::patch-2.8-hb03c661_1002 \n",
      "  python_abi         conda-forge/noarch::python_abi-3.10-8_graalpy230_310_native \n",
      "  sysroot_linux-64   conda-forge/noarch::sysroot_linux-64-2.28-h4ee821c_8 \n",
      "  zstd               conda-forge/linux-64::zstd-1.5.7-hb8e6e7a_2 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  libgcc-ng          pkgs/main::libgcc-ng-11.2.0-h1234567_1 --> conda-forge::libgcc-ng-15.2.0-h69a702a_7 \n",
      "  libgomp              pkgs/main::libgomp-11.2.0-h1234567_1 --> conda-forge::libgomp-15.2.0-h767d61c_7 \n",
      "  libstdcxx-ng       pkgs/main::libstdcxx-ng-11.2.0-h12345~ --> conda-forge::libstdcxx-ng-15.2.0-h4852527_7 \n",
      "  openssl              pkgs/main::openssl-1.1.1w-h7f8727e_0 --> conda-forge::openssl-3.5.4-h26f9b46_0 \n",
      "  python                pkgs/main::python-3.10.6-haa1d7c7_1 --> conda-forge::python-3.10.8-0_native230_graalpy \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ld_impl_linux-64   pkgs/main::ld_impl_linux-64-2.44-h153~ --> conda-forge::ld_impl_linux-64-2.39-hcc3a1bd_1 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "graalpy-23.0.0       | 326.8 MB  |                                       |   0% \n",
      "gcc_impl_linux-64-10 | 46.7 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "binutils_impl_linux- | 4.7 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-15.2.0     | 3.7 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-devel_linux-6 | 3.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.5.4        | 3.0 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "krb5-1.21.3          | 1.3 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kernel-headers_linux | 1.2 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-15.2.0        | 803 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ld_impl_linux-64-2.3 | 675 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libnghttp2-1.67.0    | 651 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zstd-1.5.7           | 554 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcurl-8.17.0       | 450 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgomp-15.2.0       | 437 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libssh2-1.11.1       | 298 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "c-ares-1.34.5        | 202 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "curl-8.17.0          | 182 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libedit-3.1.20250104 | 132 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "keyutils-1.6.3       | 131 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   |                                       |   0% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   |                                       |   0% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | 3                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | 1                                     |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | 2                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | 1                                     |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | #2                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | 1                                     |   0% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | 4                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | #5                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | 8                                     |   2% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | ##3                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | 1                                     |   1% \u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | 7                                     |   2% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | 9                                     |   2% \u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | 4                                     |   1% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #                                     |   3% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | #####1                                |  14% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | #####7                                |  15% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ##1                                   |   6% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #1                                    |   3% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | 4                                     |   1% \u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ##8                                   |   8% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  |                                       |   0% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | 6                                     |   2% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | 8                                     |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #8                                    |   5% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | 9                                     |   3% \u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ###1                                  |   8% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##                                    |   6% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | #########2                            |  25% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #1                                    |   3% \u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ####8                                 |  13% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##3                                   |   6% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | ###########5                          |  31% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #2                                    |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##5                                   |   7% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | #############5                        |  37% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 1                                     |   0% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #4                                    |   4% \u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | #####3                                |  14% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###5                                  |  10% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ########                              |  22% \u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #5                                    |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | ###############2                      |  41% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | #########                             |  25% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###8                                  |  10% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 1                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #6                                    |   4% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ####2                                 |  11% \u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 2                                     |   1% \u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ##########7                           |  29% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | ##################5                   |  50% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 2                                     |   1% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | #####################5                |  58% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ###########6                          |  31% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ####5                                 |  12% \u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 2                                     |   1% \u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ############9                         |  35% \u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 2                                     |   1% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #####                                 |  14% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | ######################9               |  62% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ###############1                      |  41% \u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##2                                   |   6% \u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ################7                     |  45% \u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 2                                     |   1% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #####5                                |  15% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ###################                   |  52% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | ########################1             |  65% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 3                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ####################3                 |  55% \u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 3                                     |   1% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #######1                              |  19% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | #####################4                |  58% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | ###########################8          |  75% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##6                                   |   7% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #######5                              |  20% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##7                                   |   7% \u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | #######################1              |  63% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 3                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #######9                              |  21% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | ##############################7       |  83% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ########################9             |  67% \u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##8                                   |   8% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | #################################9    |  92% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ########1                             |  22% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | #########################8            |  70% \u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 3                                     |   1% \u001b[A\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 4                                     |   1% \u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 4                                     |   1% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###2                                  |   9% \u001b[A\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 4                                     |   1% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ########8                             |  24% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###3                                  |   9% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##########                            |  27% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 4                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ###############################2      |  85% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 5                                     |   1% \u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##########8                           |  29% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###4                                  |   9% \u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ###################################4  |  96% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###########1                          |  30% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###7                                  |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ####################################6 |  99% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###########5                          |  31% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###8                                  |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 5                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###9                                  |  11% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    | ####4                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###########8                          |  32% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 5                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ####2                                 |  12% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ############4                         |  34% \u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ############7                         |  35% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "binutils_impl_linux- | 4.7 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    | #######4                              |  20% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ####3                                 |  12% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ####4                                 |  12% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #############1                        |  35% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    | ###########6                          |  31% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "binutils_impl_linux- | 4.7 MB    | ####1                                 |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #############7                        |  37% \u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 6                                     |   2% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    | ##############9                       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "binutils_impl_linux- | 4.7 MB    | ########                              |  22% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 6                                     |   2% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##############4                       |  39% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "binutils_impl_linux- | 4.7 MB    | ###########2                          |  30% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##############8                       |  40% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ####8                                 |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 6                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #####                                 |  14% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###############1                      |  41% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    | ####################6                 |  56% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #####1                                |  14% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###############4                      |  42% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #####2                                |  14% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    | #########################4            |  69% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###############8                      |  43% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "binutils_impl_linux- | 4.7 MB    | ###################5                  |  53% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ################2                     |  44% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #####4                                |  15% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    | ############################2         |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "binutils_impl_linux- | 4.7 MB    | ###########################9          |  76% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ################5                     |  45% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-devel_linu | 9.6 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "binutils_impl_linux- | 4.7 MB    | ##############################9       |  84% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 7                                     |   2% \u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #################2                    |  47% \u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #################6                    |  48% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    | ##################################4   |  93% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "gxx_impl_linux-64-10 | 11.5 MB   | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##################                    |  49% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #####8                                |  16% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##################4                   |  50% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-15.2.0     | 3.7 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-devel_linux-6 | 3.3 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ######4                               |  17% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "binutils_impl_linux- | 4.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 8                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-devel_linux-6 | 3.3 MB    | 5                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-15.2.0     | 3.7 MB    | 4                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##################7                   |  51% \u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###################5                  |  53% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ######5                               |  18% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-15.2.0     | 3.7 MB    | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ######7                               |  18% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###################9                  |  54% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-devel_linux-6 | 3.3 MB    | ####1                                 |  11% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-15.2.0     | 3.7 MB    | #####1                                |  14% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | 9                                     |   3% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ####################3                 |  55% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libsanitizer-10.4.0  | 6.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-15.2.0     | 3.7 MB    | #######1                              |  19% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ####################8                 |  56% \u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #####################2                |  57% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 9                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #######1                              |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-devel_linux-6 | 3.3 MB    | ########################5             |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 9                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #######5                              |  21% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-devel_linux-6 | 3.3 MB    | ################################1     |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-15.2.0     | 3.7 MB    | ########################8             |  67% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #######8                              |  21% \u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | 9                                     |   3% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-devel_linux-6 | 3.3 MB    | ####################################1 |  98% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-15.2.0     | 3.7 MB    | ##############################3       |  82% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ######################                |  60% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ########                              |  22% \u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #                                     |   3% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-15.2.0     | 3.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libstdcxx-15.2.0     | 3.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.5.4        | 3.0 MB    | 1                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #                                     |   3% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ########4                             |  23% \u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #                                     |   3% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.5.4        | 3.0 MB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-devel_linux-6 | 3.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #######################4              |  63% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ########6                             |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.5.4        | 3.0 MB    | ########5                             |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ########9                             |  24% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #######################8              |  64% \u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #1                                    |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #1                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.5.4        | 3.0 MB    | ####################2                 |  55% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ########################7             |  67% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.5.4        | 3.0 MB    | ############################7         |  78% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "krb5-1.21.3          | 1.3 MB    | ##########6                           |  29% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #1                                    |   3% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #########4                            |  26% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "krb5-1.21.3          | 1.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "krb5-1.21.3          | 1.3 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.5.4        | 3.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "openssl-3.5.4        | 3.0 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kernel-headers_linux | 1.2 MB    | 4                                     |   1% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-15.2.0        | 803 KB    | 7                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #########################9            |  70% \u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###########################           |  73% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kernel-headers_linux | 1.2 MB    | #4                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #2                                    |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###########################4          |  74% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #########7                            |  26% \u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #2                                    |   3% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kernel-headers_linux | 1.2 MB    | ##########                            |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kernel-headers_linux | 1.2 MB    | #####################9                |  59% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #2                                    |   3% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##########2                           |  28% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | #############################6        |  80% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-15.2.0        | 803 KB    | ########8                             |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #2                                    |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-15.2.0        | 803 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgcc-15.2.0        | 803 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##########8                           |  29% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libnghttp2-1.67.0    | 651 KB    | 9                                     |   2% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #3                                    |   4% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##############################5       |  82% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libnghttp2-1.67.0    | 651 KB    | #8                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###########                           |  30% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ld_impl_linux-64-2.3 | 675 KB    | ##########################3           |  71% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kernel-headers_linux | 1.2 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "kernel-headers_linux | 1.2 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ld_impl_linux-64-2.3 | 675 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #3                                    |   4% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###########5                          |  31% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###############################6      |  86% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zstd-1.5.7           | 554 KB    | #                                     |   3% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###########7                          |  32% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libnghttp2-1.67.0    | 651 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libnghttp2-1.67.0    | 651 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #4                                    |   4% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zstd-1.5.7           | 554 KB    | ##1                                   |   6% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #4                                    |   4% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###########8                          |  32% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zstd-1.5.7           | 554 KB    | ########5                             |  23% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ############                          |  33% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zstd-1.5.7           | 554 KB    | ###################################2  |  95% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcurl-8.17.0       | 450 KB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #4                                    |   4% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "zstd-1.5.7           | 554 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###################################2  |  95% \u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #5                                    |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcurl-8.17.0       | 450 KB    | ##########5                           |  28% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ###################################8  |  97% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ############4                         |  34% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcurl-8.17.0       | 450 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcurl-8.17.0       | 450 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ####################################3 |  98% \u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #5                                    |   4% \u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ####################################8 | 100% \u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #5                                    |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgomp-15.2.0       | 437 KB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #5                                    |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgomp-15.2.0       | 437 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libgomp-15.2.0       | 437 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ############8                         |  35% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libssh2-1.11.1       | 298 KB    | #9                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #5                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libssh2-1.11.1       | 298 KB    | #########9                            |  27% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #############                         |  35% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #6                                    |   4% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libssh2-1.11.1       | 298 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #6                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "c-ares-1.34.5        | 202 KB    | ##############6                       |  40% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "c-ares-1.34.5        | 202 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "c-ares-1.34.5        | 202 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #############6                        |  37% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #############9                        |  38% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libedit-3.1.20250104 | 132 KB    | ####5                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "keyutils-1.6.3       | 131 KB    | ####5                                 |  12% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libedit-3.1.20250104 | 132 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #6                                    |   5% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #7                                    |   5% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #7                                    |   5% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##############5                       |  39% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "curl-8.17.0          | 182 KB    | ##########################            |  70% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "curl-8.17.0          | 182 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "keyutils-1.6.3       | 131 KB    | #########                             |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "keyutils-1.6.3       | 131 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##############6                       |  40% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "graalpy-23.0.0       | 326.8 MB  | #8                                    |   5% [A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "sysroot_linux-64-2.2 | 23.1 MB   | ##################################### | 100% \u001b[A\u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##############8                       |  40% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###############                       |  41% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###############3                      |  41% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #9                                    |   5% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###############5                      |  42% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###############8                      |  43% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #9                                    |   5% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #9                                    |   5% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #9                                    |   5% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##                                    |   6% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ################7                     |  45% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##                                    |   6% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##1                                   |   6% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##1                                   |   6% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #################2                    |  47% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #################3                    |  47% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #################5                    |  48% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##2                                   |   6% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #################9                    |  49% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##2                                   |   6% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##3                                   |   6% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##3                                   |   6% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##################4                   |  50% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##4                                   |   7% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##5                                   |   7% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##5                                   |   7% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##5                                   |   7% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###################                   |  52% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###################2                  |  52% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##7                                   |   7% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###################5                  |  53% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##7                                   |   8% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ####################1                 |  55% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##8                                   |   8% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ##9                                   |   8% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ####################6                 |  56% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ####################7                 |  56% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###                                   |   8% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###1                                  |   8% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #####################2                |  57% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###1                                  |   9% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #####################7                |  59% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###3                                  |   9% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###3                                  |   9% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###4                                  |   9% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ######################4               |  61% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###5                                  |  10% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ######################8               |  62% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###6                                  |  10% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###6                                  |  10% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###7                                  |  10% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###7                                  |  10% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###7                                  |  10% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###8                                  |  10% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###9                                  |  11% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ###9                                  |  11% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####                                  |  11% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####                                  |  11% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ########################6             |  67% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ########################9             |  67% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####1                                 |  11% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####2                                 |  11% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####2                                 |  12% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #########################6            |  69% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####4                                 |  12% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #########################9            |  70% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####4                                 |  12% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####5                                 |  12% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##########################6           |  72% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####6                                 |  13% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###########################1          |  73% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###########################2          |  74% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####7                                 |  13% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###########################5          |  74% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####7                                 |  13% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ############################2         |  76% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####8                                 |  13% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ####8                                 |  13% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####                                 |  14% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #############################         |  78% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #############################4        |  80% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####                                 |  14% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####2                                |  14% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####2                                |  14% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##############################6       |  83% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####3                                |  14% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####4                                |  15% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###############################4      |  85% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###############################8      |  86% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####5                                |  15% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####6                                |  15% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####6                                |  15% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ################################7     |  88% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####7                                |  16% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | #################################4    |  90% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####8                                |  16% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ##################################    |  92% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #####9                                |  16% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ######1                               |  16% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ######1                               |  17% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ######1                               |  17% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ###################################7  |  97% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ######2                               |  17% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ####################################3 |  98% \u001b[A\n",
      "gcc_impl_linux-64-10 | 46.7 MB   | ####################################5 |  99% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | ########4                             |  23% \u001b[A\n",
      "graalpy-23.0.0       | 326.8 MB  | #########################5            |  69% ^CA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "CondaSignalInterrupt: Signal interrupt SIGINT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge gxx_linux-64=10.4.0 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a41c16b-bfdf-4658-953f-5c6d9dfaf434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - nvidia/label/cuda-11.4.0\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /root/miniconda3/envs/llama1\n",
      "\n",
      "  added / updated specs:\n",
      "    - cuda-toolkit\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    cuda-cccl-11.4.43          |                0         1.1 MB  nvidia/label/cuda-11.4.0\n",
      "    cuda-command-line-tools-11.4.0|                0           1 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-compiler-11.4.0       |                0           1 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-cudart-11.4.43        |                0         190 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-cudart-dev-11.4.43    |                0         963 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-cuobjdump-11.4.43     |                0         115 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-cupti-11.4.65         |                0        20.8 MB  nvidia/label/cuda-11.4.0\n",
      "    cuda-cuxxfilt-11.4.43      |                0         262 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-driver-dev-11.4.43    |                0          15 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-gdb-11.4.55           |                0         4.7 MB  nvidia/label/cuda-11.4.0\n",
      "    cuda-libraries-11.4.0      |                0           1 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-libraries-dev-11.4.0  |                0           2 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-memcheck-11.4.43      |                0         153 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nsight-11.4.43        |                0       113.6 MB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nsight-compute-11.4.0 |                0           1 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nvcc-11.4.48          |                0        54.6 MB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nvdisasm-11.4.43      |                0        31.6 MB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nvml-dev-11.4.43      |                0          62 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nvprof-11.4.43        |                0         4.3 MB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nvprune-11.4.43       |                0          46 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nvrtc-11.4.50         |                0        16.9 MB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nvrtc-dev-11.4.50     |                0          10 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nvtx-11.4.43          |                0          43 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-nvvp-11.4.43          |                0       114.3 MB  nvidia/label/cuda-11.4.0\n",
      "    cuda-sanitizer-api-11.4.54 |                0        14.8 MB  nvidia/label/cuda-11.4.0\n",
      "    cuda-toolkit-11.4.0        |                0           1 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-tools-11.4.0          |                0           1 KB  nvidia/label/cuda-11.4.0\n",
      "    cuda-visual-tools-11.4.0   |                0           1 KB  nvidia/label/cuda-11.4.0\n",
      "    expat-2.7.3                |       h3385a95_0         167 KB\n",
      "    gds-tools-1.0.0.82         |                0        39.5 MB  nvidia/label/cuda-11.4.0\n",
      "    libcublas-11.5.2.43        |                0       239.8 MB  nvidia/label/cuda-11.4.0\n",
      "    libcublas-dev-11.5.2.43    |                0       250.6 MB  nvidia/label/cuda-11.4.0\n",
      "    libcufft-10.5.0.43         |                0       250.2 MB  nvidia/label/cuda-11.4.0\n",
      "    libcufft-dev-10.5.0.43     |                0       514.9 MB  nvidia/label/cuda-11.4.0\n",
      "    libcufile-1.0.0.82         |                0         470 KB  nvidia/label/cuda-11.4.0\n",
      "    libcufile-dev-1.0.0.82     |                0        10.4 MB  nvidia/label/cuda-11.4.0\n",
      "    libcurand-10.2.5.43        |                0        48.8 MB  nvidia/label/cuda-11.4.0\n",
      "    libcurand-dev-10.2.5.43    |                0        49.6 MB  nvidia/label/cuda-11.4.0\n",
      "    libcusolver-11.2.0.43      |                0       128.7 MB  nvidia/label/cuda-11.4.0\n",
      "    libcusolver-dev-11.2.0.43  |                0        40.0 MB  nvidia/label/cuda-11.4.0\n",
      "    libcusparse-11.6.0.43      |                0       153.0 MB  nvidia/label/cuda-11.4.0\n",
      "    libcusparse-dev-11.6.0.43  |                0       307.5 MB  nvidia/label/cuda-11.4.0\n",
      "    libnpp-11.4.0.33           |                0       106.6 MB  nvidia/label/cuda-11.4.0\n",
      "    libnpp-dev-11.4.0.33       |                0       103.8 MB  nvidia/label/cuda-11.4.0\n",
      "    libnsl-2.0.0               |       h5eee18b_0          31 KB\n",
      "    libnvjpeg-11.5.1.43        |                0         2.2 MB  nvidia/label/cuda-11.4.0\n",
      "    libnvjpeg-dev-11.5.1.43    |                0         1.8 MB  nvidia/label/cuda-11.4.0\n",
      "    nsight-compute-2021.2.0.15 |                0       327.7 MB  nvidia/label/cuda-11.4.0\n",
      "    openssl-3.0.18             |       hd6dcaed_0         4.5 MB\n",
      "    python-3.10.19             |       h6fa692b_0        24.5 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        2.91 GB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  cuda-cccl          nvidia/label/cuda-11.4.0/linux-64::cuda-cccl-11.4.43-0 \n",
      "  cuda-command-line~ nvidia/label/cuda-11.4.0/linux-64::cuda-command-line-tools-11.4.0-0 \n",
      "  cuda-compiler      nvidia/label/cuda-11.4.0/linux-64::cuda-compiler-11.4.0-0 \n",
      "  cuda-cudart        nvidia/label/cuda-11.4.0/linux-64::cuda-cudart-11.4.43-0 \n",
      "  cuda-cudart-dev    nvidia/label/cuda-11.4.0/linux-64::cuda-cudart-dev-11.4.43-0 \n",
      "  cuda-cuobjdump     nvidia/label/cuda-11.4.0/linux-64::cuda-cuobjdump-11.4.43-0 \n",
      "  cuda-cupti         nvidia/label/cuda-11.4.0/linux-64::cuda-cupti-11.4.65-0 \n",
      "  cuda-cuxxfilt      nvidia/label/cuda-11.4.0/linux-64::cuda-cuxxfilt-11.4.43-0 \n",
      "  cuda-driver-dev    nvidia/label/cuda-11.4.0/linux-64::cuda-driver-dev-11.4.43-0 \n",
      "  cuda-gdb           nvidia/label/cuda-11.4.0/linux-64::cuda-gdb-11.4.55-0 \n",
      "  cuda-libraries     nvidia/label/cuda-11.4.0/linux-64::cuda-libraries-11.4.0-0 \n",
      "  cuda-libraries-dev nvidia/label/cuda-11.4.0/linux-64::cuda-libraries-dev-11.4.0-0 \n",
      "  cuda-memcheck      nvidia/label/cuda-11.4.0/linux-64::cuda-memcheck-11.4.43-0 \n",
      "  cuda-nsight        nvidia/label/cuda-11.4.0/linux-64::cuda-nsight-11.4.43-0 \n",
      "  cuda-nsight-compu~ nvidia/label/cuda-11.4.0/linux-64::cuda-nsight-compute-11.4.0-0 \n",
      "  cuda-nvcc          nvidia/label/cuda-11.4.0/linux-64::cuda-nvcc-11.4.48-0 \n",
      "  cuda-nvdisasm      nvidia/label/cuda-11.4.0/linux-64::cuda-nvdisasm-11.4.43-0 \n",
      "  cuda-nvml-dev      nvidia/label/cuda-11.4.0/linux-64::cuda-nvml-dev-11.4.43-0 \n",
      "  cuda-nvprof        nvidia/label/cuda-11.4.0/linux-64::cuda-nvprof-11.4.43-0 \n",
      "  cuda-nvprune       nvidia/label/cuda-11.4.0/linux-64::cuda-nvprune-11.4.43-0 \n",
      "  cuda-nvrtc         nvidia/label/cuda-11.4.0/linux-64::cuda-nvrtc-11.4.50-0 \n",
      "  cuda-nvrtc-dev     nvidia/label/cuda-11.4.0/linux-64::cuda-nvrtc-dev-11.4.50-0 \n",
      "  cuda-nvtx          nvidia/label/cuda-11.4.0/linux-64::cuda-nvtx-11.4.43-0 \n",
      "  cuda-nvvp          nvidia/label/cuda-11.4.0/linux-64::cuda-nvvp-11.4.43-0 \n",
      "  cuda-sanitizer-api nvidia/label/cuda-11.4.0/linux-64::cuda-sanitizer-api-11.4.54-0 \n",
      "  cuda-toolkit       nvidia/label/cuda-11.4.0/linux-64::cuda-toolkit-11.4.0-0 \n",
      "  cuda-tools         nvidia/label/cuda-11.4.0/linux-64::cuda-tools-11.4.0-0 \n",
      "  cuda-visual-tools  nvidia/label/cuda-11.4.0/linux-64::cuda-visual-tools-11.4.0-0 \n",
      "  expat              pkgs/main/linux-64::expat-2.7.3-h3385a95_0 \n",
      "  gds-tools          nvidia/label/cuda-11.4.0/linux-64::gds-tools-1.0.0.82-0 \n",
      "  libcublas          nvidia/label/cuda-11.4.0/linux-64::libcublas-11.5.2.43-0 \n",
      "  libcublas-dev      nvidia/label/cuda-11.4.0/linux-64::libcublas-dev-11.5.2.43-0 \n",
      "  libcufft           nvidia/label/cuda-11.4.0/linux-64::libcufft-10.5.0.43-0 \n",
      "  libcufft-dev       nvidia/label/cuda-11.4.0/linux-64::libcufft-dev-10.5.0.43-0 \n",
      "  libcufile          nvidia/label/cuda-11.4.0/linux-64::libcufile-1.0.0.82-0 \n",
      "  libcufile-dev      nvidia/label/cuda-11.4.0/linux-64::libcufile-dev-1.0.0.82-0 \n",
      "  libcurand          nvidia/label/cuda-11.4.0/linux-64::libcurand-10.2.5.43-0 \n",
      "  libcurand-dev      nvidia/label/cuda-11.4.0/linux-64::libcurand-dev-10.2.5.43-0 \n",
      "  libcusolver        nvidia/label/cuda-11.4.0/linux-64::libcusolver-11.2.0.43-0 \n",
      "  libcusolver-dev    nvidia/label/cuda-11.4.0/linux-64::libcusolver-dev-11.2.0.43-0 \n",
      "  libcusparse        nvidia/label/cuda-11.4.0/linux-64::libcusparse-11.6.0.43-0 \n",
      "  libcusparse-dev    nvidia/label/cuda-11.4.0/linux-64::libcusparse-dev-11.6.0.43-0 \n",
      "  libnpp             nvidia/label/cuda-11.4.0/linux-64::libnpp-11.4.0.33-0 \n",
      "  libnpp-dev         nvidia/label/cuda-11.4.0/linux-64::libnpp-dev-11.4.0.33-0 \n",
      "  libnsl             pkgs/main/linux-64::libnsl-2.0.0-h5eee18b_0 \n",
      "  libnvjpeg          nvidia/label/cuda-11.4.0/linux-64::libnvjpeg-11.5.1.43-0 \n",
      "  libnvjpeg-dev      nvidia/label/cuda-11.4.0/linux-64::libnvjpeg-dev-11.5.1.43-0 \n",
      "  nsight-compute     nvidia/label/cuda-11.4.0/linux-64::nsight-compute-2021.2.0.15-0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  libffi                                     3.3-he6710b0_2 --> 3.4.4-h6a678d5_1 \n",
      "  openssl                                 1.1.1w-h7f8727e_0 --> 3.0.18-hd6dcaed_0 \n",
      "  python                                  3.10.6-haa1d7c7_1 --> 3.10.19-h6fa692b_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "libcufft-dev-10.5.0. | 514.9 MB  |                                       |   0% \n",
      "nsight-compute-2021. | 327.7 MB  |                                       |   0% \u001b[A\n",
      "\n",
      "libcusparse-dev-11.6 | 307.5 MB  |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "libcublas-dev-11.5.2 | 250.6 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "libcufft-10.5.0.43   | 250.2 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcublas-11.5.2.43  | 239.8 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcusparse-11.6.0.4 | 153.0 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcusolver-11.2.0.4 | 128.7 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cuda-nvvp-11.4.43    | 114.3 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cuda-nsight-11.4.43  | 113.6 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libnpp-11.4.0.33     | 106.6 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libnpp-dev-11.4.0.33 | 103.8 MB  |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cuda-nvcc-11.4.48    | 54.6 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcurand-dev-10.2.5 | 49.6 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcurand-10.2.5.43  | 48.8 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcusolver-dev-11.2 | 40.0 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "gds-tools-1.0.0.82   | 39.5 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cuda-nvdisasm-11.4.4 | 31.6 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "python-3.10.19       | 24.5 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cuda-cupti-11.4.65   | 20.8 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cuda-nvrtc-11.4.50   | 16.9 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "cuda-sanitizer-api-1 | 14.8 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "libcufile-dev-1.0.0. | 10.4 MB   |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " ... (more hidden) ...\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "!conda install -c \"nvidia/label/cuda-11.4.0\" cuda-toolkit -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefeeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install \"fastapi[all]\" uvicorn -i https://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97d9386e-1450-4eed-a25b-0c033845f600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Collecting huggingface-hub\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/db/fb/d71f914bc69e6357cbde04db62ef15497cd27926d95f03b4930997c4c390/huggingface_hub-1.0.1-py3-none-any.whl (503 kB)\n",
      "Collecting filelock (from huggingface-hub)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/76/91/7216b27286936c16f5b4d0c530087e4a54eead683e6b0b73dd0c64844af6/filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/eb/02/a6b21098b1d5d6249b7c5ab69dde30108a71e4e819d4a9778f1de1d5b70d/fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from huggingface-hub) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from huggingface-hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from huggingface-hub) (6.0.3)\n",
      "Collecting shellingham (from huggingface-hub)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/e0/f9/0595336914c5619e5f28a1fb793285925a8cd4b432c9da0a987836c7f822/shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface-hub)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting typer-slim (from huggingface-hub)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/5e/dd/5cbf31f402f1cc0ab087c94d4669cfa55bd1e818688b910631e131d74e75/typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from huggingface-hub) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/9a/92/cf3ab0b652b082e66876d08da57fcc6fa2f0e6c70dfbbafbd470bb73eb47/hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: anyio in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub) (4.11.0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub) (1.0.9)\n",
      "Requirement already satisfied: idna in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/envs/llama/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub) (1.3.1)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub)\n",
      "  Downloading https://mirrors.aliyun.com/pypi/packages/db/d3/9dcc0f5797f070ec8edf30fbadfb200e71d9db6b84d211e3b2085a7589a0/click-8.3.0-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: tqdm, shellingham, hf-xet, fsspec, filelock, click, typer-slim, huggingface-hub\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [huggingface-hub] [huggingface-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed click-8.3.0 filelock-3.20.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-1.0.1 shellingham-1.5.4 tqdm-4.67.1 typer-slim-0.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install huggingface-hub -i https://mirrors.aliyun.com/pypi/simple/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4454b06f-6ce3-4f4c-9828-a992d7dd0640",
   "metadata": {},
   "outputs": [
    {
     "ename": "LocalEntryNotFoundError",
     "evalue": "An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpx/_transports/default.py:101\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpcore/_sync/connection.py:78\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     ssl_object \u001b[38;5;241m=\u001b[39m stream\u001b[38;5;241m.\u001b[39mget_extra_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mssl_object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpcore/_sync/connection.py:124\u001b[0m, in \u001b[0;36mHTTPConnection._connect\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnect_tcp\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m--> 124\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpcore/_backends/sync.py:207\u001b[0m, in \u001b[0;36mSyncBackend.connect_tcp\u001b[0;34m(self, host, port, timeout, local_address, socket_options)\u001b[0m\n\u001b[1;32m    202\u001b[0m exc_map: ExceptionMapping \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    203\u001b[0m     socket\u001b[38;5;241m.\u001b[39mtimeout: ConnectTimeout,\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[1;32m    205\u001b[0m }\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    208\u001b[0m     sock \u001b[38;5;241m=\u001b[39m socket\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[1;32m    209\u001b[0m         address,\n\u001b[1;32m    210\u001b[0m         timeout,\n\u001b[1;32m    211\u001b[0m         source_address\u001b[38;5;241m=\u001b[39msource_address,\n\u001b[1;32m    212\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 101] Network is unreachable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:1602\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1601\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1602\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RemoteEntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:89\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_legacy_arguments(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:1528\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[1;32m   1527\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1528\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43m_httpx_follow_relative_redirects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m hf_raise_for_status(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:283\u001b[0m, in \u001b[0;36m_httpx_follow_relative_redirects\u001b[0;34m(method, url, **httpx_kwargs)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# Make the request\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhttpx_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m429\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     hf_raise_for_status(response)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:402\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around httpx to retry calls on an endpoint, with exponential backoff.\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03mEndpoint call is retried on exceptions (ex: connection timeout, proxy error,...)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m> issue on [Github](https://github.com/huggingface/huggingface_hub).\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_http_backoff_base\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/utils/_http.py:312\u001b[0m, in \u001b[0;36m_http_backoff_base\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, stream, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_retry(response):\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpx/_client.py:825\u001b[0m, in \u001b[0;36mClient.request\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    812\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[1;32m    813\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    814\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    823\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[1;32m    824\u001b[0m )\n\u001b[0;32m--> 825\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpx/_transports/default.py:249\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/contextlib.py:153\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/httpx/_transports/default.py:118\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mConnectError\u001b[0m: [Errno 101] Network is unreachable",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m repo_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoMonir/Meta-Llama-3-8B-Instruct-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./meta-llama-3-8b-instruct.Q6_K.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型已下载至: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:89\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         validate_repo_id(arg_value)\n\u001b[1;32m     87\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_legacy_arguments(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:991\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, dry_run)\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[1;32m    972\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m    973\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    988\u001b[0m         dry_run\u001b[38;5;241m=\u001b[39mdry_run,\n\u001b[1;32m    989\u001b[0m     )\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 991\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m    995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1003\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:1117\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, dry_run)\u001b[0m\n\u001b[1;32m   1114\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1117\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m etag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124metag must have been retrieved from server\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/llama/lib/python3.10/site-packages/huggingface_hub/file_download.py:1713\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1714\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1715\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1716\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1717\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhead_call_error\u001b[39;00m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"MoMonir/Meta-Llama-3-8B-Instruct-GGUF\"\n",
    "filename = \"./meta-llama-3-8b-instruct.Q6_K.gguf\"\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=repo_id,\n",
    "    filename=filename\n",
    ")\n",
    "\n",
    "print(f\"模型已下载至: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e15cb2b-c933-43c2-864c-8d88a89611e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 3 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n",
      "  Device 1: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n",
      "  Device 2: NVIDIA GeForce RTX 2080 Ti, compute capability 7.5, VMM: yes\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 2080 Ti) - 10771 MiB free\n",
      "llama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 2080 Ti) - 10771 MiB free\n",
      "llama_model_load_from_file_impl: using device CUDA2 (NVIDIA GeForce RTX 2080 Ti) - 10771 MiB free\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from ./meta-llama-3-8b-instruct.Q6_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  19:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q6_K\n",
      "print_info: file size   = 6.14 GiB (6.56 BPW) \n",
      "load: missing pre-tokenizer type, using: 'default'\n",
      "load:                                             \n",
      "load: ************************************        \n",
      "load: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "load: CONSIDER REGENERATING THE MODEL             \n",
      "load: ************************************        \n",
      "load:                                             \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128255 '<|reserved_special_token_250|>' is not marked as EOG\n",
      "load: control token: 128254 '<|reserved_special_token_249|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_248|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128010 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128004 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128008 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 128001 ('<|end_of_text|>')\n",
      "load:   - 128009 ('<|eot_id|>')\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.8000 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Meta-Llama-3-8B-Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: EOG token        = 128001 '<|end_of_text|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CUDA0, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CUDA1, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: layer  25 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: layer  27 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: layer  28 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: layer  29 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: layer  31 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: layer  32 assigned to device CUDA2, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors:        CUDA0 model buffer size =  1877.22 MiB\n",
      "load_tensors:        CUDA1 model buffer size =  1877.22 MiB\n",
      "load_tensors:        CUDA2 model buffer size =  2117.55 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   410.98 MiB\n",
      "........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 2048\n",
      "llama_context: n_ctx_per_seq = 2048\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 500000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "create_memory: n_ctx = 2048 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   1: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   2: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   3: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   4: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   5: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   6: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   7: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   8: dev = CUDA0\n",
      "llama_kv_cache_unified: layer   9: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  10: dev = CUDA0\n",
      "llama_kv_cache_unified: layer  11: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  12: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  13: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  14: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  15: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  16: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  17: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  18: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  19: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  20: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  21: dev = CUDA1\n",
      "llama_kv_cache_unified: layer  22: dev = CUDA2\n",
      "llama_kv_cache_unified: layer  23: dev = CUDA2\n",
      "llama_kv_cache_unified: layer  24: dev = CUDA2\n",
      "llama_kv_cache_unified: layer  25: dev = CUDA2\n",
      "llama_kv_cache_unified: layer  26: dev = CUDA2\n",
      "llama_kv_cache_unified: layer  27: dev = CUDA2\n",
      "llama_kv_cache_unified: layer  28: dev = CUDA2\n",
      "llama_kv_cache_unified: layer  29: dev = CUDA2\n",
      "llama_kv_cache_unified: layer  30: dev = CUDA2\n",
      "llama_kv_cache_unified: layer  31: dev = CUDA2\n",
      "llama_kv_cache_unified:      CUDA0 KV buffer size =    88.00 MiB\n",
      "llama_kv_cache_unified:      CUDA1 KV buffer size =    88.00 MiB\n",
      "llama_kv_cache_unified:      CUDA2 KV buffer size =    80.00 MiB\n",
      "llama_kv_cache_unified: size =  256.00 MiB (  2048 cells,  32 layers,  1/1 seqs), K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 4\n",
      "llama_context: max_nodes = 2328\n",
      "llama_context: pipeline parallelism enabled (n_copies=4)\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      CUDA0 compute buffer size =   224.02 MiB\n",
      "llama_context:      CUDA1 compute buffer size =   224.02 MiB\n",
      "llama_context:      CUDA2 compute buffer size =   322.53 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =    40.04 MiB\n",
      "llama_context: graph nodes  = 1126\n",
      "llama_context: graph splits = 4\n",
      "CUDA : USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '18', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# 加载模型，n_gpu_layers=-1 表示将所有可卸载的层都放到 GPU 上\n",
    "# 如果显存不足，可以适当调低这个数值，例如 20 或 30\n",
    "try:\n",
    "    llm = Llama(\n",
    "      model_path=\"./meta-llama-3-8b-instruct.Q6_K.gguf\",\n",
    "      n_gpu_layers=-1, # 将所有层卸载到 GPU\n",
    "      n_ctx=2048,      # 上下文窗口大小\n",
    "      verbose=True     # 打印详细信息\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"加载模型失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5269d877-ec4b-4e57-bffc-e04c60974779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    5353.76 ms\n",
      "llama_perf_context_print: prompt eval time =      71.00 ms /     8 tokens (    8.88 ms per token,   112.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   31783.75 ms /  2039 runs   (   15.59 ms per token,    64.15 tokens per second)\n",
      "llama_perf_context_print:       total time =   36478.29 ms /  2047 tokens\n",
      "llama_perf_context_print:    graphs reused =       1975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",并在服务器上创建一个简单的Web应用程序。\n",
      "## 1.创建HTTP服务器\n",
      "使用C#语言可以使用`System.Net`命名空间中的`HttpListener`类来创建一个HTTP服务器。\n",
      "```csharp\n",
      "using System;\n",
      "using System.Net;\n",
      "using System.IO;\n",
      "\n",
      "class MyHttpServer\n",
      "{\n",
      "    static void Main(string[] args)\n",
      "    {\n",
      "        // 创建一个HttpListener对象\n",
      "        HttpListener listener = new HttpListener();\n",
      "        listener.Prefixes.Add(\"http://localhost:8080/\");\n",
      "\n",
      "        // 启动服务器\n",
      "        listener.Start();\n",
      "\n",
      "        // 等待请求\n",
      "        Console.WriteLine(\"Server started. Listening on port 8080...\");\n",
      "        while (true)\n",
      "        {\n",
      "            HttpListenerContext context = listener.GetContext();\n",
      "            HttpListenerRequest request = context.Request;\n",
      "\n",
      "            // 处理请求\n",
      "            string responseText = \"Hello, World!\";\n",
      "            byte[] responseBytes = System.Text.Encoding.UTF8.GetBytes(responseText);\n",
      "            context.Response.ContentLength64 = responseBytes.Length;\n",
      "            Stream responseStream = context.Response.OutputStream;\n",
      "            responseStream.Write(responseBytes, 0, responseBytes.Length);\n",
      "            responseStream.Close();\n",
      "        }\n",
      "    }\n",
      "}\n",
      "```\n",
      "在上面的代码中，我们创建了一个`HttpListener`对象，并添加了一个前缀`http://localhost:8080/`，表示服务器监听的URL。然后，我们启动服务器，使用`GetContext()`方法等待请求。每当收到一个请求时，我们将处理请求，生成一个响应，包括响应文本和响应体，然后将响应发送回客户端。\n",
      "\n",
      "## 2.创建简单的Web应用程序\n",
      "创建一个简单的Web应用程序，可以使用HTML、CSS和JavaScript语言编写。下面是一个简单的Web应用程序的示例：\n",
      "```html\n",
      "<!-- index.html -->\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>My Web Application</title>\n",
      "    <style>\n",
      "        body {\n",
      "            font-family: Arial, sans-serif;\n",
      "            text-align: center;\n",
      "        }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>Welcome to My Web Application!</h1>\n",
      "    <p>This is a simple web application.</p>\n",
      "    <script>\n",
      "        // JavaScript代码\n",
      "        console.log(\"Hello, World!\");\n",
      "    </script>\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "在上面的代码中，我们创建了一个简单的HTML文件，包括头部、主体和脚本部分。头部部分定义了页面的标题和样式，主体部分包含了一个标题和一个段落，脚本部分包含了一个JavaScript代码。\n",
      "\n",
      "## 3.将Web应用程序部署到HTTP服务器\n",
      "将Web应用程序部署到HTTP服务器，可以使用File System的方法将HTML文件和其他资源文件复制到服务器的文件系统中。\n",
      "\n",
      "例如，我们可以将index.html文件复制到服务器的文件系统中，例如：\n",
      "```csharp\n",
      "// 将index.html文件复制到服务器的文件系统中\n",
      "File.Copy(\"index.html\", @\"C:\\inetpub\\wwwroot\\index.html\");\n",
      "```\n",
      "然后，我们可以使用HttpListener对象来服务请求，例如：\n",
      "```csharp\n",
      "// 服务请求\n",
      "HttpListenerContext context = listener.GetContext();\n",
      "HttpListenerRequest request = context.Request;\n",
      "\n",
      "// 处理请求\n",
      "string responseText = File.ReadAllText(@\"C:\\inetpub\\wwwroot\\index.html\");\n",
      "byte[] responseBytes = System.Text.Encoding.UTF8.GetBytes(responseText);\n",
      "context.Response.ContentLength64 = responseBytes.Length;\n",
      "Stream responseStream = context.Response.OutputStream;\n",
      "responseStream.Write(responseBytes, 0, responseBytes.Length);\n",
      "responseStream.Close();\n",
      "```\n",
      "在上面的代码中，我们使用File类来读取index.html文件，然后将其内容发送回客户端。\n",
      "\n",
      "## 4.测试Web应用程序\n",
      "测试Web应用程序，可以使用浏览器或其他HTTP客户端工具来访问服务器。例如，我们可以使用Google Chrome浏览器来访问服务器，例如：\n",
      "```\n",
      "http://localhost:8080/index.html\n",
      "```\n",
      "在浏览器中，我们可以看到Web应用程序的内容，包括标题、段落和JavaScript代码。\n",
      "\n",
      "## 5.总结\n",
      "在本文中，我们使用C#语言创建了一个HTTP服务器，并将简单的Web应用程序部署到服务器上。我们使用HttpListener对象来服务请求，并将响应发送回客户端。我们还使用File类来读取Web应用程序的资源文件，并将其内容发送回客户端。最后，我们使用浏览器来测试Web应用程序。希望这篇文章能够帮助您了解如何使用C#语言创建一个简单的Web应用程序。 ## 6.其他资源\n",
      "\n",
      "* [System.Net.HttpListener Class](https://docs.microsoft.com/en-us/dotnet/api/system.net.httplistener?view=net-5.0)\n",
      "* [C# Http Server Example](https://www.codeproject.com/Articles/10441/C-HTTP-Server-Example)\n",
      "* [Building a Simple Web Server in C#](https://www.codeproject.com/Articles/11531/Building-a-Simple-Web-Server-in-Csharp)\n",
      "\n",
      "这些资源提供了更多关于HttpListener类和C# Http服务器的信息和示例代码。 ## 7.结论\n",
      "\n",
      "在本文中，我们使用C#语言创建了一个简单的HTTP服务器，并将Web应用程序部署到服务器上。我们使用HttpListener对象来服务请求，并将响应发送回客户端。希望这篇文章能够帮助您了解如何使用C#语言创建一个简单的Web应用程序。 ## 8.更多资源\n",
      "\n",
      "* [C# Programming](https://www.csharp.net/)\n",
      "* [ASP.NET](https://docs.microsoft.com/en-us/aspnet/)\n",
      "* [System.Net.HttpListener Class](https://docs.microsoft.com/en-us/dotnet/api/system.net.httplistener?view=net-5.0)\n",
      "\n",
      "这些资源提供了更多关于C#语言、ASP.NET和HttpListener类的信息和示例代码。 ## 9.结论\n",
      "\n",
      "在本文中，我们使用C#语言创建了一个简单的HTTP服务器，并将Web应用程序部署到服务器上。我们使用HttpListener对象来服务请求，并将响应发送回客户端。希望这篇文章能够帮助您了解如何使用C#语言创建一个简单的Web应用程序。 # 10.致谢\n",
      "\n",
      "感谢您阅读本文！如果您有任何问题或需要更多帮助，请随时联系我。 # 11.参考\n",
      "\n",
      "[1] System.Net.HttpListener Class. (n.d.). Retrieved from <https://docs.microsoft.com/en-us/dotnet/api/system.net.httplistener?view=net-5.0>\n",
      "\n",
      "[2] C# Http Server Example. (n.d.). Retrieved from <https://www.codeproject.com/Articles/10441/C-HTTP-Server-Example>\n",
      "\n",
      "[3] Building a Simple Web Server in C#. (n.d.). Retrieved from <https://www.codeproject.com/Articles/11531/Building-a-Simple-Web-Server-in-Csharp>\n",
      "\n",
      "[4] C# Programming. (n.d.). Retrieved from <https://www.csharp.net/>\n",
      "\n",
      "[5] ASP.NET. (n.d.). Retrieved from <https://docs.microsoft.com/en-us/aspnet/>\n",
      "\n",
      "[6] System.Net.HttpListener Class. (n.d.). Retrieved from <https://docs.microsoft.com/en-us/dotnet/api/system.net.httplistener?view=net-5.0>\n",
      "\n",
      "# 12.结束语\n",
      "\n",
      "本文为您提供了一个简单的C# Http服务器的示例代码，展示了如何使用HttpListener对象来服务请求，并将响应发送回客户端。我们还介绍了如何部署Web应用程序到服务器上，并使用浏览器来测试Web应用程序。如果您需要更多帮助或有任何问题，请随时联系我。 # 13.后记\n",
      "\n",
      "本文的示例代码可以在Visual Studio中编译和运行。您可以将代码复制到Visual Studio中，然后使用F5键来启动服务器和测试Web应用程序。如果您需要更多帮助或有任何问题，请随时联系我。 # 14.致谢\n",
      "\n",
      "感谢您阅读本文！如果您有任何问题或需要更多帮助，请随时联系我。 # 15.参考\n",
      "\n",
      "[1] System.Net.HttpListener Class. (n.d.). Retrieved from <https://docs.microsoft.com/en-us/dotnet/api/system.net.httplistener?view=net-5.0>\n",
      "\n",
      "[2] C# Http Server Example. (n.d.). Retrieved from <https://www.codeproject.com/Articles/10441/C-HTTP-Server-Example>\n",
      "\n",
      "[3] Building a Simple Web Server in C#. (n.d.). Retrieved from <https://www.codeproject.com/Articles/11531/Building-a-Simple-Web-Server-in-Csharp>\n",
      "\n",
      "[4] C# Programming. (n.d.). Retrieved from <https://www.csharp.net/>\n",
      "\n",
      "[5] ASP.NET. (n.d.). Retrieved from <https://docs.microsoft.com/en-us/aspnet/>\n",
      "\n",
      "[6] System.Net.HttpListener Class. (n.d.). Retrieved from <https://docs.microsoft.com/en-us/dotnet/api/system.net.httplistener?view=net-5.0>\n",
      "\n",
      "# 16.结束语\n",
      "\n",
      "本文为您提供了一个简单的C# Http服务器的示例代码，展示了如何使用HttpListener对象来服务请求，并将响应发送回客户端。我们还介绍了如何部署Web应用程序到服务器上，并使用浏览器来测试Web应用程序。如果您需要更多帮助或有任何问题，请随时联系我\n"
     ]
    }
   ],
   "source": [
    "# 创建一个简单的推理请求\n",
    "if 'llm' in locals():\n",
    "    try:\n",
    "        output = llm(\n",
    "              \"使用c#编写一个http服务器\",\n",
    "              max_tokens=2048,\n",
    "              stop=[\"<|eot_id|>\"], # Llama 3 的停止符\n",
    "              echo=False\n",
    "        )\n",
    "        print(output[\"choices\"][0][\"text\"])\n",
    "    except Exception as e:\n",
    "        print(f\"模型推理失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d1f7d-cd51-4812-b804-0db5551959f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
